{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://mars.nasa.gov/#news_and_events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Year of Surprising Science From NASA's InSight Mars Mission\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_title = (soup.find(\"div\", class_=\"list_text\")).find(\"h3\").text\n",
    "print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # BONUS: The news_p was something that didn't come through because of page loading. You might have to try this with Selenium or Splinter\n",
    "\n",
    "# url = \"https://mars.nasa.gov/#news_and_events\"\n",
    "# driver = webdriver.Firefox()\n",
    "# driver.get(url)\n",
    "# time.sleep(1)\n",
    "\n",
    "\n",
    "# elem = driver.find_element_by_partial_link_text(f'{news_title}')\n",
    "# elem.click()\n",
    "\n",
    "# elem = browser.find_element_by_id(\"full_image\")\n",
    "# elem.click()\n",
    "# browser.find_element_by_link_text(\"more info\")\n",
    "# #new_url = elem.get_attribute(\"href\")\n",
    "# print(new_url)\n",
    "\n",
    "\n",
    "# browser = webdriver.Firefox()\n",
    "# browser.get(url)\n",
    "\n",
    "# browser.find_element_by_id(\"full_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA07137_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Connect to a url to grab NASA's featured Mars image\n",
    "\n",
    "# Use selenium to navegate to the page with the image\n",
    "driver = webdriver.Firefox()\n",
    "url_image = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "driver.get(url_image)\n",
    "time.sleep(1)\n",
    "\n",
    "elem = driver.find_element_by_id(\"full_image\")\n",
    "elem.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Navigate to \"more info\" web site, where the full image URL is\n",
    "elem = driver.find_element_by_link_text(\"more info\")\n",
    "elem.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Get the \"more info\" page URL to parse and find the image in full resolution\n",
    "url_more_info = driver.current_url\n",
    "\n",
    "# Use requests and soup to return the image url\n",
    "r = requests.get(url_more_info)\n",
    "html = r.text\n",
    "soup = bs(html, \"html.parser\")\n",
    "featured_image_url = (\n",
    "    \"https://www.jpl.nasa.gov\" + soup.find(\"figure\", class_=\"lede\").a[\"href\"]\n",
    ")\n",
    "print(featured_image_url)\n",
    "\n",
    "# Close the browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather today in Mars is: \n",
      "InSight sol 444 (2020-02-25) low -93.8ºC (-136.8ºF) high -12.0ºC (10.5ºF)\n",
      "winds from the SSW at 6.2 m/s (13.9 mph) gusting to 21.2 m/s (47.4 mph)\n",
      "pressure at 6.30 hPapic.twitter.com/UeOmoDjhf3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mars Weather\n",
    "# Visit https://twitter.com/marswxreport?lang=en and scrape the latest Mars weather tweet from the page.\n",
    "# Save the tweet text for the weather report as a variable called `mars_weather`.\n",
    "\n",
    "# Connect to url for Mars Weather Twitter page\n",
    "\n",
    "url_twitter = \"https://twitter.com/marswxreport?lang=en\"\n",
    "\n",
    "# Create a Beautiful Soup object\n",
    "r = requests.get(url_twitter)\n",
    "html = r.text\n",
    "soup = bs(html, \"html.parser\")\n",
    "mars_weather = soup.find_all(\"div\", class_=\"js-tweet-text-container\")\n",
    "\n",
    "# find the first actual weather tweet\n",
    "\n",
    "mars_weather = mars_weather[0].text\n",
    "\n",
    "print(f\"The weather today in Mars is: {mars_weather}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "lxml not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9d76dbb6e6a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://space-facts.com/mars/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmars_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_facts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayed_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0mretained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflav\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parser_dispatch\u001b[0;34m(flavor)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAS_LXML\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lxml not found, please install it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_valid_parsers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: lxml not found, please install it"
     ]
    }
   ],
   "source": [
    "## Mars Facts - use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "#  Use Pandas to convert the data to a HTML table string.\n",
    "\n",
    "url_facts = \"http://space-facts.com/mars/\"\n",
    "mars_facts = pd.read_html(url_facts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Hemispheres\n",
    "\n",
    "# set url and the driver\n",
    "driver = webdriver.Firefox()\n",
    "url_hem = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "driver.get(url_hem)\n",
    "\n",
    "# scrape page into Soup\n",
    "r = requests.get(url_hem)\n",
    "html = r.text\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "\n",
    "# Scrape the images of each hemisphere\n",
    "hemisphere_image_urls = []\n",
    "hemispheres = [\"Cerberus\", \"Schiaparelli\", \"Syrtis Major\", \"Valles Marineris\"]\n",
    "# find hemisphere titles\n",
    "# hemispheres = soup.find_all(\"h3\")\n",
    "# print(hemispheres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the four pages containing the information\n",
    "for hemisphere in hemispheres:\n",
    "    h = driver.find_elements_by_partial_link_text(hemisphere)\n",
    "    h.click()\n",
    "    time.sleep(1)\n",
    "    #     html = driver.html\n",
    "    #     soup = bs(html, 'html.parser')\n",
    "    #     title = soup.find('h3', class_='title').text\n",
    "    #     img_url = 'https://web.archive.org/' + soup.find('img', class_='wide-image')['src']\n",
    "    #     hemisphere_image_urls.append({'title': title, 'img_url': img_url})\n",
    "    #     driver.back()\n",
    "    #     time.sleep(1)\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "mars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
